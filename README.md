# RobArmSL
A 6-DOF Robotics Arm control for English to American Sign Language translation
![image](https://github.com/user-attachments/assets/3254c910-835a-4b09-87bb-117195ce57fe)

# Project Overview
To mitigate the
complexity of communication between the deaf and dumb and
common beings, a two-way communication and interpretation
model has been designed to translate the English language into
ASL symbols on the robotic arm and ASL into the English
language using image recognition. Along with translation, the
model can also analyse the sentiments of the mute people and
has an auto-text completion feature for ease of typing, thus
improving the entire calibre of interaction and making it more
human-like.
# Operation
 This is an efficient, portable, low-cost model
for the two-way communication system. Currently, the
design is composed of three Machine learning image
recognition models for recognizing ASL letters shown to the
webcam and displaying corresponding English language
alphabets on the user-interface, which in this case is the
laptop screen. This completes one-way communication
between the mute person and the common person.
The other way of communication is established when a
person, who does not know ASL wants to convey a message
to a deaf and dumb person, he/she can type the English
alphabet on the keyboard and a 3D printed robotic arm will
show the corresponding ASL symbol for the letter.
